<p align="center">
  <img src="./assets/logo.png" width="900" alt="TextPixs Logo Placeholder"/>
</p>

# üé®‚ú® TextPixs: Enhancing Text Rendering in Text-to-Image Generation: A Novel Approach

### <div align="center"> Final Year Project 2025 <div>

<div align="center">
  <a href="https://github.com/SyedaAnshrahGillani/TextPixs"><img src="https://img.shields.io/static/v1?label=Project&message=Github&color=blue&logo=github"></a> &ensp;
  <a href="https://arxiv.org/abs/XXXX.XXXXX"><img src="https://img.shields.io/static/v1?label=Paper&message=Arxiv&color=red&logo=arxiv"></a> &ensp;
  <!-- Add more badges as needed, e.g., demo link, Hugging Face Space -->
</div>

<p align="center" border-radius="10px">
  <img src="./assets/results.png" width="90%" alt="TextPixs Results Image"/>
</p>

## üé• Demo Video

[![Watch the video](./Demo%20recording/Thumbnail_demo.png)](https://drive.google.com/file/d/1b8f2pu2OKTieu2JUkgql-7dF8zCXXSBu/view?usp=sharing)

*(Clicking on the thumbnail will open the demo video in a new tab.)*

## Abstract / Executive Summary

The most recent efflorescence of text-to-image diffusion models has brought an entire new epoch to digital content creation, showing a mind-boggling capacity to generate photorealistic and artistically variable imagery based on the semantics of natural language descriptions. Nonetheless, a high and constant drawback of such models has always been the inability to produce a coherent, legible, and properly spelt text in generated images, which majorly stiffens applicability of such models in various real life instances such as advertising, education, and even creative design. This paper addresses this critical shortcoming by introducing a novel, multi-pronged framework designed to seamlessly integrate orthographic precision into the generative process while maintaining high-quality image synthesis capabilities. Our proposed methodology, Glyph-Conditioned Diffusion with Character-Aware Attention (GCDA), enhances a standard diffusion backbone with three synergistic and carefully designed components: (1) a dual-stream text encoder that processes both semantic contextual information and explicit visual glyph representations to create a rich, character-aware embedding of the input text; (2) a character-aware attention mechanism guided by a novel attention segregation loss that encourages the model to allocate distinct spatial focus to individual characters, preventing fusion and distortion artifacts; and (3) an OCR-in-the-loop fine-tuning stage that employs a comprehensive text perceptual loss to directly optimize for legibility and spelling accuracy. Using large-scale experimentation on standard benchmark data such as MARIO-10M and T2I-CompBench, our model achieves a new state of the art in all the developed evaluation metrics within a model, with significantly better results in character-based measures of text-rendering (Character Error Rate: 0.08 vs 0.21 of those found in previous best, Word Error Rate: 0.15 vs 0.25) and human evaluations, as well as competitive image s... [truncated]

## üî• Changelog

- **[2025-07-06] - Current Update**
    - Updated README with comprehensive details, including author sequence, demo video link, and refined project structure.
    - Renamed `image_generator` to `textpixs_image_generator` for clarity and branding.
    - Ensured `.env` files are ignored by Git and removed from repository history.
    - Re-added `gradio_demo` folder to ensure its presence on the remote branch.
- **[2025-07-03]**
    - README updated to professional standards, inspired by leading research projects.
    - FYP1 deliverables organized and integrated into the repository.
- **[2025-06-30]**
    - Initial project setup and repository creation.

## Table of Contents üìö

- [Abstract / Executive Summary](#abstract--executive-summary)
- [Changelog](#-changelog)
- [Introduction](#introduction)
- [Features](#features)
- [Technologies Used](#technologies-used)
- [Getting Started](#getting-started)
- [Roadmap](#roadmap)
- [Performance](#performance)
- [Team](#team)
- [FYP1 Deliverables](#fyp1-deliverables)
- [Project Structure](#project-structure)
- [To-Do List](#to-do-list)
- [Contact](#contact)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgements](#acknowledgements)
- [Citation](#citation)

## üí° Introduction

The field of text-to-image generation (T2I) has seen remarkable progress, with models like DALL-E and Imagen generating high-fidelity images from natural language descriptions. However, a persistent and critical challenge remains: the accurate and legible rendering of textual content within these generated images. This limitation significantly impacts the applicability of T2I models in domains requiring precise text integration, such as advertising, educational content, and digital art.

This project, TextPixs, introduces a novel, multi-pronged framework designed to overcome this fundamental hurdle. Our methodology, Glyph-Conditioned Diffusion with Character-Aware Attention (GCDA), enhances a standard diffusion backbone by integrating three synergistic components:

1.  **Dual-Stream Text Encoder**: This innovative encoder processes input text through two parallel streams:
        -   A **Semantic Stream** (utilizing a pre-trained BERT encoder) captures high-level meaning, context, and relationships.
        -   An **Orthographic Stream** (employing a Glyph Renderer and a Character-Level CNN) extracts precise visual representations of text structure, character shapes, and typographic information.
        This dual-stream approach ensures that the model understands both *what* the text means and *how* it visually appears, overcoming the limitations of traditional tokenization.
2.  **Character-Aware Attention Mechanism**: This mechanism, guided by a novel **Attention Segregation Loss**, explicitly teaches the model to allocate distinct, non-overlapping spatial attention to individual characters during the generation process. This prevents common issues like character fusion and distortion, ensuring legible and well-formed text.
3.  **OCR-in-the-Loop Fine-Tuning**: This stage employs a comprehensive text perceptual loss, leveraging Optical Character Recognition (OCR)-based feedback loops to directly optimize for legibility and spelling accuracy. This iterative refinement process significantly enhances text fidelity.

Through extensive experimentation on standard benchmarks such as MARIO-10M and T2I-CompBench, TextPixs achieves state-of-the-art results in character-based text rendering metrics (e.g., Character Error Rate: 0.08, Word Error Rate: 0.15) and high exact match accuracy (75.4%) while maintaining competitive image synthesis quality (FID: 14.3). Our comprehensive ablation studies validate the necessity and synergistic effect of each component. TextPixs represents a substantial advancement towards developing T2I models that are not only visually creative and aesthetically pleasing but also linguistically competent and practically deployable.

## Features üöÄ

TextPixs is engineered with a suite of powerful features designed to push the boundaries of text-to-image generation, focusing on high-fidelity text rendering:

- **Glyph-Conditioned Diffusion with Character-Aware Attention (GCDA) Framework**: At its core, TextPixs employs a novel hybrid architecture built upon a standard diffusion backbone, integrating three synergistic components:
    - **Dual-Stream Text Encoder**: This innovative encoder processes input text through two parallel streams:
        -   A **Semantic Stream** (utilizing a pre-trained BERT encoder) captures high-level meaning, context, and relationships.
        -   An **Orthographic Stream** (employing a Glyph Renderer and a Character-Level CNN) extracts precise visual representations of text structure, character shapes, and typographic information.
        This dual-stream approach ensures that the model understands both *what* the text means and *how* it visually appears, overcoming the limitations of traditional tokenization.
    -   **Character-Aware Attention Mechanism**: This mechanism, guided by a novel **Attention Segregation Loss**, explicitly teaches the model to allocate distinct, non-overlapping spatial attention to individual characters during the generation process. This prevents common issues like character fusion and distortion, ensuring legible and well-formed text.
    -   **OCR-in-the-Loop Fine-Tuning**: This stage employs a comprehensive text perceptual loss, leveraging Optical Character Recognition (OCR)-based feedback loops to directly optimize for legibility and spelling accuracy. This iterative refinement process significantly enhances text fidelity.

- **Enhanced Text-to-Image Generation Capabilities**:
    -   **Unprecedented Text Fidelity**: Achieves state-of-the-art results in character-based text rendering metrics (e.g., Character Error Rate: 0.08, Word Error Rate: 0.15) and high exact match accuracy (75.4%).
    -   **Maintained Image Quality**: Despite the focus on text, the framework maintains competitive image synthesis quality (FID: 14.3), ensuring visually stunning and coherent images.
    -   **Dual-Stage Refinement Pipeline**: A coarse-to-fine generation process ensures intricate textual details are captured with exceptional clarity.

- **Future Enhancements & Visionary Capabilities**:
    -   **Image Modifications (Phase 2 - Upcoming)**: Empowering users to modify existing images using intuitive text prompts for advanced customization.
    -   **Text-to-Video Capabilities (Future Phase - Visionary)**: Generating short, dynamic videos from descriptive prompts, expanding AI-driven content creation horizons.
    -   **Advanced Optimizations**: Continuous enhancements include dynamic learning rate adjustment, multilingual training support, and attention map visualization for deeper model insights.

## Technologies Used üõ°Ô∏è

TextPixs leverages a robust and cutting-edge stack of technologies to deliver its advanced capabilities in text-to-image generation:

- **Core Framework**: Our primary implementation is in Python, utilizing:
    -   **PyTorch**: For high-performance deep learning, enabling the development and training of our custom models.
    -   **Hugging Face Transformers & Diffusers**: Leveraging pre-trained models and libraries for efficient implementation of diffusion models and transformer architectures.

- **AI/ML Models**: At the heart of TextPixs are:
    -   **Fine-tuned Stable Diffusion Models**: Serving as the generative backbone, adapted for enhanced text rendering.
    -   **Custom Character-Level CNNs**: Specifically designed for the orthographic stream of our dual-stream encoder, processing visual glyph representations.
    -   **Pre-trained Optical Character Recognition (OCR) Models (e.g., TrOCR)**: Utilized in the OCR-in-the-loop fine-tuning stage for robust text legibility and accuracy feedback.
    -   **BERT Language Models**: Employed in the semantic stream of our dual-stream encoder for capturing high-level textual meaning and context.

- **Datasets**: Training and evaluation are conducted using a comprehensive and diverse set of datasets, including:
    -   **MARIO-10M**: A large-scale dataset for text-in-image generation.
    -   **T2I-CompBench**: A benchmark specifically designed for compositional text-to-image generation with text rendering evaluation protocols.
    -   **COCO-Text & TextCaps**: Datasets rich in text-image pairs for general text-to-image tasks.
    -   **Custom Multilingual Datasets**: Developed to ensure broad applicability and robust performance across various languages and scripts.

- **Development & Deployment**: While the core research focuses on model development, the project also utilizes:
    -   **Gradio**: For creating an intuitive and interactive web-based demo, facilitating easy access and experimentation with TextPixs.
    -   **Git & GitHub Actions**: For version control, collaborative development, and automated workflows.

## üöÄ Getting Started

To set up and run the Gradio demo for TextPixs, follow these steps:

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/SyedaAnshrahGillani/TextPixs.git
    cd TextPixs
    ```

2.  **Navigate to the Gradio demo directory**:
    ```bash
    cd app
    ```

3.  **Create and activate a virtual environment** (recommended):
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    ```

4.  **Install the required dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

5.  **Run the Gradio application**:
    ```bash
    python app.py
    ```

    The application will typically run on `http://127.0.0.1:7860/` or a similar local address. You can access the demo through your web browser.

    *(Note: A GPU is required to run the demo. Ensure you have Python 3.8+ installed on your system.)*

## Roadmap üó∫Ô∏è

Our project roadmap is structured into distinct phases, with iterative progress towards key milestones:

- **Phase 1: Foundational Research & Design (July 2024 - January 2025)**:
    -   Comprehensive literature review and gap analysis in text-to-image generation.
    -   Detailed design of the Glyph-Conditioned Diffusion with Character-Aware Attention (GCDA) framework.
    -   Initial implementation of dual-stream text encoder and character-aware attention mechanisms.

- **Phase 2: Model Development & Refinement (February 2025 - May 2025)**:
    -   Integration of OCR-in-the-loop fine-tuning for enhanced text fidelity.
    -   Extensive experimentation and ablation studies on benchmark datasets (MARIO-10M, T2I-CompBench).
    -   Performance optimization and hyperparameter tuning.

- **Phase 3: Evaluation, Documentation & Dissemination (June 2025 - July 2025)**:
    -   Rigorous quantitative and qualitative evaluation of TextPixs.
    -   Preparation and submission of the final research paper to CVPR 2025.
    -   Development of comprehensive project documentation and final FYP report.
    -   Release of Gradio demo for public access and community engagement.

## Performance üìä

Our proposed method, TextPixs, demonstrates significant improvements over existing text-to-image generation models, particularly in text fidelity and visual quality. The following table summarizes the performance comparison based on key metrics, highlighting our state-of-the-art results:

| Model | FID (‚Üì) | BLEU (‚Üë) | OCR Accuracy (‚Üë) | Character Error Rate (CER ‚Üì) | Word Error Rate (WER ‚Üì) | Exact Match Accuracy (Acc. ‚Üë) |
|---|---|---|---|---|---|---|
| DALL-E | 12.34 | 0.72 | 65% | - | - | - |
| Imagen | 10.56 | 0.81 | 72% | - | - | - |
| TextDiffuser-2 | 14.1 | 0.71 | 60.1% | 0.14 | 0.25 | 60.1% |
| **TextPixs (Proposed)** | **9.12** | **0.89** | **85%** | **0.08** | **0.15** | **75.4%** |

**Key Performance Highlights:**

-   **Superior Text Rendering**: TextPixs achieves a Character Error Rate (CER) of **0.08**, a significant **43% improvement** over the previous best (TextDiffuser-2 at 0.14). Our Word Error Rate (WER) of **0.15** also demonstrates superior word-level accuracy.
-   **High Exact Match Accuracy**: We achieve **75.4% exact match accuracy**, indicating that a substantial majority of generated text is perfectly rendered and recognized by OCR systems. This is a **15.3% absolute improvement** over TextDiffuser-2.
-   **Maintained Image Quality**: Despite the strong focus on text, the framework maintains competitive image synthesis quality (FID: 14.3), ensuring visually stunning and coherent images.
-   **Robustness and Consistency**: Our ablation studies confirm that all components of the GCDA framework are essential and contribute synergistically to the overall performance. The low standard deviations across multiple evaluation runs demonstrate the stability and reliability of our approach.

## Team üë•

**Authors**:
Syeda Anshrah Gillani<sup>1*</sup>, Mirza Samad Ahmed Baig<sup>3</sup>, Osama Ahmed Khan<sup>1</sup>, Shahid Munir Shah<sup>1</sup>, Umeema Mujeeb<sup>1</sup>, Maheen Ali<sup>1</sup>

## FYP1 Deliverables üì¶

The following key deliverables from FYP1 are available in the `docs/FYP1-Deliverables` directory:

- **FYP 1 Details**: `FYP 1 Details - TextPixs.pdf`
- **Gap Analysis**: `Gap Analysis - Text Pixs.pdf`
- **Project Plan (Gantt Chart)**: `Project Plan - Text Pixs - GANTT CHART ~ FYP ~ Anshrah.pdf`
- **Report - Latex - 3 Chapters**: `Report - Latex - 3 Chapters - TextPixs .pdf`
- **Research Paper Draft - Latex**: `Research Paper Draft - Latex - Text Pixs.pdf`
- **Selection of Publication Venue**: `Selection of Publication Venue - TextPixs - Latex.pdf`
- **Sign Off Sheet**: `Sign Off Sheet - Text Pixs.pdf`
- **FYP1 Presentation**: `TextPixs FYP1.pdf`
- **Standee Design**: `Textpixs-STANDEE-3.pdf`

## FYP2 Deliverables üöÄ

The following key deliverables from FYP2 are available in the `docs/FYP2-Deliverables` directory:

- **CVPR 2025 Submission**: `CVPR 2025 Submission #1234. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE..pdf`
- **Demo Video (Compressed)**: `demo1_compressed.mp4`
- **Simulation and Experimental Results Section**: `Simulation and:or Experimental Results Section.pdf`

## Project Structure üìÅ

The repository is organized as follows:

```
TextPixs/
‚îú‚îÄ‚îÄ .DS_Store
‚îú‚îÄ‚îÄ .git/
‚îú‚îÄ‚îÄ .gitattributes
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ *Final Research Paper/
‚îú‚îÄ‚îÄ *Standee/
‚îú‚îÄ‚îÄ Demo recording/
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ app/
‚îú‚îÄ‚îÄ assets/
‚îú‚îÄ‚îÄ docs/
‚îú‚îÄ‚îÄ path/
‚îî‚îÄ‚îÄ training_data/
```

## üí™ To-Do List

We are actively working on and planning the following:

- [ ] Refine and optimize the Semantic Alignment Module.
- [ ] Further develop and evaluate the Text Rendering Loss function.
- [ ] Enhance the Dual-Stage Refinement pipeline for improved image quality.
- [ ] Conduct more extensive experiments with dynamic learning rate adjustment.
- [ ] Expand multilingual training support to additional languages.
- [ ] Develop advanced Attention Map Visualization techniques.
- [ ] Explore new datasets for training and evaluation.
- [ ] Prepare the final research paper for submission to a top-tier conference (e.g., CVPR).
- [ ] Implement and integrate the image modification feature (Phase 2).
- [ ] Explore and develop text-to-video capabilities (Future Phase).
- [ ] Prepare comprehensive documentation and the final FYP report.

## Contact üìß

For any inquiries or collaborations, please contact:

- **Syeda Anshrah Gillani (Group Leader)**: syedaanshrah16@gmail.com

## Contributing ü§ù

We welcome contributions! If you‚Äôre interested in improving TextPixs, please fork the repository, create a new branch, and submit a pull request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/your-feature-name`)
3. Commit your changes (`git commit -m 'Add new feature'`)
4. Push to the branch (`git push origin feature/your-feature-name`)
5. Open a pull request

## License üìú

Distributed under the MIT License. See `LICENSE` for more information.

## ü§ó Acknowledgements

We extend our gratitude to the following:

- Our supervisors, Sir Osama Ahmed Khan and Mirza Samad Ahmed Baig, for their invaluable guidance and support.
- Mirza Samad Ahmed Baig for his sponsorship and continued commitment to the project.
- All researchers and open-source contributors whose work has inspired and facilitated this project.

## üìñ Citation

If you find this research project helpful, please consider citing our work (details to be provided upon paper submission).

```bibtex
@article{gillani2025enhancing,
  title={Enhancing Text Rendering in Text-to-Image Generation: A Novel Approach},
  author={Gillani, Syeda Anshrah Gillani, Mirza Samad Ahmed and Khan, Osama Ahmed Khan, Shahid Munir Shah, Umema Mujeeb, Maheen Ali},
  journal={Final Research paper submitted for evaluation CVPR 2025 submission},
  year={2025}
}
```
Copyright (c) 2025 Syeda Anshrah Gillani
All rights reserved.

This software is proprietary and confidential.
No part of this codebase may be copied, modified, distributed,
published, or used in whole or in part without the prior written
permission of the copyright holder.
